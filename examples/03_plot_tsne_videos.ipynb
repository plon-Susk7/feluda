{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7218a582-481f-4167-aa51-aef8d2a38659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m19 packages\u001b[0m \u001b[2min 1.41s\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m17 packages\u001b[0m \u001b[2min 1.50s\u001b[0m\u001b[0m                                            \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m17 packages\u001b[0m \u001b[2min 27ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mboto3\u001b[0m\u001b[2m==1.35.96\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbotocore\u001b[0m\u001b[2m==1.35.96\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2024.12.14\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdacite\u001b[0m\u001b[2m==1.8.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfeluda\u001b[0m\u001b[2m==0.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjmespath\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydub\u001b[0m\u001b[2m==0.25.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1ms3transfer\u001b[0m\u001b[2m==0.10.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtoml\u001b[0m\u001b[2m==0.10.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwerkzeug\u001b[0m\u001b[2m==3.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwget\u001b[0m\u001b[2m==3.2\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m39 packages\u001b[0m \u001b[2min 1.26s\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m30 packages\u001b[0m \u001b[2min 2m 07s\u001b[0m\u001b[0m                                           \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m30 packages\u001b[0m \u001b[2min 97ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfeluda-vid-vec-rep-clip\u001b[0m\u001b[2m==0.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.16.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2024.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.27.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.21.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2024.11.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.20.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.47.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.1.0\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m6 packages\u001b[0m \u001b[2min 775ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m5 packages\u001b[0m \u001b[2min 2.99s\u001b[0m\u001b[0m                                             \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfeluda-dimension-reduction\u001b[0m\u001b[2m==0.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.15.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mthreadpoolctl\u001b[0m\u001b[2m==3.5.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install feluda \n",
    "!uv pip install feluda-vid-vec-rep-clip \n",
    "!uv pip install feluda-dimension-reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f88913-4c1f-45f8-9541-1237b342c851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: feluda\n",
      "Version: 0.9.1\n",
      "Location: /home/priyash7/Desktop/feluda/examples/.venv/lib/python3.10/site-packages\n",
      "Requires: boto3, dacite, pydub, pyyaml, requests, toml, werkzeug, wget\n",
      "Required-by:\n",
      "Name: feluda-vid-vec-rep-clip\n",
      "Version: 0.1.1\n",
      "Location: /home/priyash7/Desktop/feluda/examples/.venv/lib/python3.10/site-packages\n",
      "Requires: pillow, torch, torchvision, transformers\n",
      "Required-by:\n",
      "Name: feluda-dimension-reduction\n",
      "Version: 0.1.1\n",
      "Location: /home/priyash7/Desktop/feluda/examples/.venv/lib/python3.10/site-packages\n",
      "Requires: numpy, scikit-learn\n",
      "Required-by:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.12 environment at /home/priyash7/Desktop/feluda/.venv\u001b[0m\n",
      "Name: feluda-vid-vec-rep-clip\n",
      "Version: 0.1.1\n",
      "Location: /home/priyash7/Desktop/feluda/.venv/lib/python3.10/site-packages\n",
      "Requires: pillow, torch, torchvision, transformers\n",
      "Required-by:\n",
      "\u001b[2mUsing Python 3.10.12 environment at /home/priyash7/Desktop/feluda/.venv\u001b[0m\n",
      "Name: feluda-dimension-reduction\n",
      "Version: 0.1.1\n",
      "Location: /home/priyash7/Desktop/feluda/.venv/lib/python3.10/site-packages\n",
      "Requires: numpy, scikit-learn\n",
      "Required-by:\n"
     ]
    }
   ],
   "source": [
    "!uv pip show feluda\n",
    "!uv pip show feluda-vid-vec-rep-clip\n",
    "!uv pip show feluda-dimension-reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071c28fb-542e-4f94-986d-932da58a3404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyash7/Desktop/feluda/examples/.venv/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import tarfile\n",
    "import cv2\n",
    "from feluda import Feluda\n",
    "import os\n",
    "from feluda.models.media_factory import VideoFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23e1718a-839e-4013-b9a5-ca8221d218a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and extracting \n",
    "\n",
    "dataset_name = \"UCF101_subset/train\"\n",
    "hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\n",
    "filename = \"UCF101_subset.tar.gz\"\n",
    "file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\")\n",
    "\n",
    "\n",
    "with tarfile.open(file_path) as t:\n",
    "     t.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9c0004f-c11d-4871-84b7-842443c8494a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/priyash7/.cache/huggingface/hub/datasets--sayakpaul--ucf101-subset/snapshots/b9984b8d2a95e4a1879e1b071e9433858d0bc24a/UCF101_subset.tar.gz'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "000db163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-SNE model successfully initialized\n",
      "Installing packages for vid_vec_rep_clip\n"
     ]
    }
   ],
   "source": [
    "feluda = Feluda(r'03_plot_tsne_videos_config.yml')\n",
    "feluda.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "022ad7dc-cc69-4313-8866-0fabfb86fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_reduction_operator = feluda.operators.get()[feluda.config.operators.parameters[0].type]\n",
    "vid_vec_clip_operator = feluda.operators.get()[feluda.config.operators.parameters[1].type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15c2a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_thumbnail(video_path, save_path):\n",
    "    \"\"\"Extract and save the first frame from the video as a thumbnail.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()  # Read the first frame\n",
    "    if ret:\n",
    "        thumbnail_path = os.path.join(save_path, os.path.basename(video_path).replace('.avi', '_thumbnail.jpg'))\n",
    "        cv2.imwrite(thumbnail_path, frame)  # Save the thumbnail as a JPEG\n",
    "        cap.release()\n",
    "        return thumbnail_path  # Return the path to the saved thumbnail\n",
    "    cap.release()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a15f2149-9172-4200-9e0d-2cc0babf570e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m thumbnail_path \u001b[38;5;241m=\u001b[39m get_video_thumbnail(video_full_path, thumbnail_save_dir)\n\u001b[1;32m     20\u001b[0m video_path \u001b[38;5;241m=\u001b[39m VideoFactory\u001b[38;5;241m.\u001b[39mmake_from_file_on_disk(\n\u001b[1;32m     21\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_name, class_dir, temp_list[i])\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mvid_vec_clip_operator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m average_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(embedding)\n\u001b[1;32m     26\u001b[0m operator_parameter\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpayload\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_name, class_dir, temp_list[i]),  \u001b[38;5;66;03m# Use the current item as the payload\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m: average_vector\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvid_vec\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     29\u001b[0m })\n",
      "File \u001b[0;32m~/Desktop/feluda/examples/.venv/lib/python3.10/site-packages/vid_vec_rep_clip.py:151\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m    149\u001b[0m fname \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     vid_analyzer \u001b[38;5;241m=\u001b[39m \u001b[43mVideoAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gendata(vid_analyzer)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/feluda/examples/.venv/lib/python3.10/site-packages/vid_vec_rep_clip.py:71\u001b[0m, in \u001b[0;36minitialize.<locals>.VideoAnalyzer.__init__\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_matrix \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/feluda/examples/.venv/lib/python3.10/site-packages/vid_vec_rep_clip.py:96\u001b[0m, in \u001b[0;36minitialize.<locals>.VideoAnalyzer.analyze\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Extract I-frames and features\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_frames(fname)\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_images\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/feluda/examples/.venv/lib/python3.10/site-packages/vid_vec_rep_clip.py:133\u001b[0m, in \u001b[0;36minitialize.<locals>.VideoAnalyzer.extract_features\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    Extracts features from a list of images using pre-trained CLIP-ViT-B-32.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Feature matrix of shape (batch, 512)\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}  \u001b[38;5;66;03m# move to device\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Desktop/feluda/examples/.venv/lib/python3.10/site-packages/transformers/models/clip/processing_clip.py:109\u001b[0m, in \u001b[0;36mCLIPProcessor.__call__\u001b[0;34m(self, text, images, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mimage_processor_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mpixel_values\n",
      "File \u001b[0;32m~/Desktop/feluda/examples/.venv/lib/python3.10/site-packages/transformers/image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/feluda/examples/.venv/lib/python3.10/site-packages/transformers/models/clip/image_processing_clip.py:286\u001b[0m, in \u001b[0;36mCLIPImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m do_convert_rgb \u001b[38;5;241m=\u001b[39m do_convert_rgb \u001b[38;5;28;01mif\u001b[39;00m do_convert_rgb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_convert_rgb\n\u001b[1;32m    284\u001b[0m validate_kwargs(captured_kwargs\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mkeys(), valid_processor_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_processor_keys)\n\u001b[0;32m--> 286\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mmake_list_of_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid_images(images):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/feluda/examples/.venv/lib/python3.10/site-packages/transformers/image_utils.py:185\u001b[0m, in \u001b[0;36mmake_list_of_images\u001b[0;34m(images, expected_ndims)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_list_of_images\u001b[39m(images, expected_ndims: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[ImageInput]:\n\u001b[1;32m    174\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    Ensure that the input is a list of images. If the input is a single image, it is converted to a list of length 1.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    If the input is a batch of images, it is converted to a list of images.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m            dimensions, an error is raised.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_batched\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m images\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# Either the input is a single image, in which case we create a list of length 1\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/feluda/examples/.venv/lib/python3.10/site-packages/transformers/image_utils.py:158\u001b[0m, in \u001b[0;36mis_batched\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_batched\u001b[39m(img):\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 158\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m is_valid_image(\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "operator_parameter = []\n",
    "sub_folder_name = os.listdir(f'{dataset_name}')\n",
    "\n",
    "for class_dir in sub_folder_name:\n",
    "    temp_list = os.listdir(os.path.join(dataset_name, class_dir))\n",
    "\n",
    "    i = 0\n",
    "    while i < len(temp_list) and i < 5:\n",
    "        if temp_list[i] == 'UCF101':\n",
    "            i += 1 \n",
    "            continue\n",
    "            \n",
    "        video_full_path = os.path.join(dataset_name, class_dir, temp_list[i])\n",
    "\n",
    "        # Extract and save the video thumbnail before processing\n",
    "        thumbnail_save_dir = 'thumbnails'\n",
    "        os.makedirs(thumbnail_save_dir, exist_ok=True) \n",
    "        thumbnail_path = get_video_thumbnail(video_full_path, thumbnail_save_dir)\n",
    "        \n",
    "        video_path = VideoFactory.make_from_file_on_disk(\n",
    "            os.path.join(dataset_name, class_dir, temp_list[i])\n",
    "        )\n",
    "        embedding = vid_vec_clip_operator.run(video_path)\n",
    "        average_vector = next(embedding)\n",
    "        \n",
    "        operator_parameter.append({\n",
    "            \"payload\": os.path.join(dataset_name, class_dir, temp_list[i]),  # Use the current item as the payload\n",
    "            \"embedding\": average_vector.get(\"vid_vec\"),\n",
    "        })\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b29b5109-2781-4353-9396-b1f3f2450ea4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input should be a non-empty list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Reducing dimension using operator\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdimension_reduction_operator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperator_parameter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/feluda/examples/.venv/lib/python3.10/site-packages/dimension_reduction.py:170\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input_data)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03mReduce the dimensionality of the provided embeddings using the initialized model.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    KeyError: If the input data is invalid.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_data, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput should be a non-empty list.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     embeddings, payloads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayload\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m input_data])\n",
      "\u001b[0;31mValueError\u001b[0m: Input should be a non-empty list."
     ]
    }
   ],
   "source": [
    "# Reducing dimension using operator\n",
    "\n",
    "data = dimension_reduction_operator.run(operator_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4a28df6-6f7c-4045-ad42-99d4bf8eb739",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m thumbnail_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthumbnails\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreduced_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m])\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreduced_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mjitter\u001b[39m(arr, jitter_amount\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "thumbnail_folder = 'thumbnails'\n",
    "\n",
    "x = np.array([item['reduced_embedding'][0] for item in data])\n",
    "y = np.array([item['reduced_embedding'][1] for item in data])\n",
    "\n",
    "def jitter(arr, jitter_amount=0.02):\n",
    "    return arr + np.random.uniform(-jitter_amount, jitter_amount, arr.shape)\n",
    "\n",
    "x_jittered = jitter(x, jitter_amount=0.3) \n",
    "y_jittered = jitter(y, jitter_amount=0.3)\n",
    "\n",
    "plt.figure(figsize=(20, 16))\n",
    "plt.scatter(x_jittered, y_jittered)\n",
    "\n",
    "def load_thumbnail(payload):\n",
    "    \"\"\"Load the thumbnail from the pre-saved thumbnail folder.\"\"\"\n",
    "    video_filename = os.path.basename(payload) \n",
    "    thumbnail_filename = video_filename.replace('.avi', '_thumbnail.jpg')\n",
    "    thumbnail_path = os.path.join(thumbnail_folder, thumbnail_filename)\n",
    "    \n",
    "    if os.path.exists(thumbnail_path):\n",
    "        return cv2.imread(thumbnail_path)\n",
    "    else:\n",
    "        print(f\"Thumbnail not found for {video_filename}\")\n",
    "    return None\n",
    "\n",
    "for i, item in enumerate(data):\n",
    "    video_thumbnail = load_thumbnail(item['payload'])\n",
    "    \n",
    "    if video_thumbnail is not None:\n",
    "        video_thumbnail = cv2.resize(video_thumbnail, (100, 100))  # Smaller thumbnails\n",
    "        video_thumbnail = cv2.cvtColor(video_thumbnail, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Keep zoom as is, no need for offsets anymore\n",
    "        imagebox = OffsetImage(video_thumbnail, zoom=0.5)\n",
    "        ab = AnnotationBbox(imagebox, (x_jittered[i], y_jittered[i]), frameon=False)  # Use jittered values\n",
    "\n",
    "        plt.gca().add_artist(ab)\n",
    "\n",
    "# Set labels and title\n",
    "plt.title('t-SNE Reduced Embeddings with Video Thumbnails')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17456d96-e3a2-4984-a0a9-bf34107e1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning cell\n",
    "\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree('thumbnails')\n",
    "shutil.rmtree('UCF101_subset')\n",
    "os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a10d5-5bae-44c5-8838-ef1efa5b9134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
